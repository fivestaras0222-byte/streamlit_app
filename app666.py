'''
cd ~/Desktop/pythonProject1/streamlit_app
streamlit run app.py
'''
from feature_order import build_and_save_feature_orders, load_feature_orders
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import joblib
import pickle
import json
import warnings
import os
# å›ºå®šéšæœºæ€§
import random
def set_random_seed(seed=42):
    np.random.seed(seed)
    random.seed(seed)
    if 'torch' in globals() and TORCH_AVAILABLE:
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_random_seed(42)
# ---- shim for main.DeepMLP so joblib can unpickle ----
import sys, types, torch
from torch import nn

class DeepMLP(nn.Module):
    def __init__(self, in_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 128), nn.BatchNorm1d(128), nn.LeakyReLU(), nn.Dropout(0.3),
            nn.Linear(128, 64),     nn.BatchNorm1d(64),  nn.LeakyReLU(), nn.Dropout(0.2),
            nn.Linear(64, 32),      nn.BatchNorm1d(32),  nn.LeakyReLU(), nn.Dropout(0.1),
            nn.Linear(32, 1)
        )
    def forward(self, x): return self.net(x)

# ensure import path 'main.DeepMLP' is available for unpickling
if 'main' in sys.modules:
    setattr(sys.modules['main'], 'DeepMLP', DeepMLP)
else:
    _m = types.ModuleType('main')
    _m.DeepMLP = DeepMLP
    sys.modules['main'] = _m
# ------------------------------------------------------

def _fail(msg: str):
    st.error(msg)
    raise RuntimeError(msg)
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥xgboost
try:
    import xgboost as xgb

    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    st.warning("âš ï¸ XGBoost is not installed.")

# å°è¯•å¯¼å…¥torch
try:
    import torch

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    st.warning("âš ï¸ PyTorch is not installed.")

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="HCC Recurrence-Free Survival Analysis Prediction System",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .model-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .risk-high { color: #d62728; font-weight: bold; }
    .risk-medium { color: #ff7f0e; font-weight: bold; }
    .risk-low { color: #2ca02c; font-weight: bold; }
    .stButton > button {
        width: 100%;
        margin: 5px 0;
    }
</style>
""", unsafe_allow_html=True)


class HCCSurvivalPredictor:

    def __init__(self):
        self.models = {}
        self.feature_mapping = self._create_feature_mapping()
        self.reverse_mapping = self._create_reverse_mapping()

        # ç»Ÿä¸€çš„ç”Ÿå­˜æ—¶é—´ç‚¹ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        # ç¤ºä¾‹ï¼šå¦‚æœè®­ç»ƒæ—¶ç”¨â€œ730å¤©â€ï¼Œå°±å†™ 730.0ï¼›å¦‚æœç”¨â€œ24ä¸ªæœˆâ€ï¼Œå°±å†™ 24.0 å¹¶åœ¨å–å€¼å¤„æŒ‰æœˆ
        self.time_horizon = 730.0

        self.load_models()
    def _coerce_estimator(self, obj):
        m = obj
        if hasattr(m, "best_estimator_"):
            m = m.best_estimator_
        if hasattr(m, "steps"):
            m = m.steps[-1][1]
        if hasattr(m, "base_estimator"):
            m = m.base_estimator
        if isinstance(m, dict):
            for k in ("model", "estimator", "best_estimator_", "final_estimator"):
                if k in m:
                    return self._coerce_estimator(m[k])
            for v in m.values():
                e = self._coerce_estimator(v)
                if hasattr(e, "predict") or hasattr(e, "predict_proba") or hasattr(e, "forward"):
                    return e
            return m
        if isinstance(m, (list, tuple)):
            for v in m:
                e = self._coerce_estimator(v)
                if hasattr(e, "predict") or hasattr(e, "predict_proba") or hasattr(e, "forward"):
                    return e
            return m
        return m

    def _load_deepsurv_joblib(self, path="models/deepsurv_model.joblib"):
        try:
            obj = joblib.load(path)
            est = self._coerce_estimator(obj)
            ok = hasattr(est, "predict") or hasattr(est, "predict_proba")
            st.sidebar.write(f"DeepSurv joblib type: {type(est).__name__}")
            st.sidebar.write(f"DeepSurv methods: {[m for m in dir(est) if m in ('predict', 'predict_proba')]}")
            return est if ok else None
        except Exception as e:
            st.sidebar.warning(f"DeepSurv joblibåŠ è½½å¤±è´¥: {e}")
            return None

        return m

    def _unwrap_estimator(self, model):
        """ä» Pipeline/CalibratedClassifierCV ç­‰åŒ…è£…ä¸­å–æœ€ç»ˆestimator"""
        m = model
        # sklearn Pipeline
        if hasattr(m, "steps"):
            m = m.steps[-1][1]
        # CalibratedClassifierCV
        if hasattr(m, "base_estimator"):
            m = m.base_estimator
        # GridSearchCV / RandomizedSearchCV
        if hasattr(m, "best_estimator_"):
            m = m.best_estimator_
        return m

    def _sigmoid(self, x):
        return 1.0 / (1.0 + np.exp(-float(x)))
    def _survival_at(self, surv_obj, t):
        """
        ä»å„ç§ç”Ÿå­˜å‡½æ•°å¯¹è±¡ä¸­å–åœ¨æ—¶é—´tçš„ç”Ÿå­˜æ¦‚ç‡S(t)ã€‚
        æ”¯æŒ:
          - å¯è°ƒç”¨å¯¹è±¡(å¦‚scikit-survivalçš„StepFunction)
          - pandas.Seriesï¼ˆindexä¸ºæ—¶é—´ï¼Œvaluesä¸ºS(t)ï¼‰
          - pandas.DataFrameï¼ˆç¬¬ä¸€åˆ—ä¸ºS(t)ï¼‰
          - ndarray/listï¼ˆæ— æ—¶é—´ç´¢å¼•åˆ™å–æœ€åä¸€ä¸ªå€¼ï¼‰
        """
        import numpy as np
        import pandas as pd

        # 1) å¯è°ƒç”¨ï¼šç›´æ¥è¯„ä¼°
        if callable(surv_obj):
            return float(surv_obj(t))

        # 2) DataFrame -> Series
        if isinstance(surv_obj, pd.DataFrame):
            if surv_obj.shape[1] == 0:
                return float('nan')
            surv_obj = surv_obj.iloc[:, 0]

        # 3) Seriesï¼šæŒ‰æ—¶é—´è½´æ‰¾ t å·¦ä¾§çš„æœ€åä¸€ä¸ªå€¼
        if isinstance(surv_obj, pd.Series):
            times = np.asarray(surv_obj.index, dtype=float)
            vals = np.asarray(surv_obj.values, dtype=float)
            if times.size == 0:
                return float('nan')
            idx = np.searchsorted(times, t, side='right') - 1
            idx = max(0, min(idx, len(vals) - 1))
            return float(vals[idx])

        # 4) ndarray/listï¼šæ²¡æœ‰æ—¶é—´è½´ï¼Œå–æœ«å€¼ï¼ˆå¸¸è§stepç»“æœï¼‰
        if isinstance(surv_obj, (list, np.ndarray)):
            arr = np.asarray(surv_obj, dtype=float)
            if arr.size == 0:
                return float('nan')
            return float(arr[-1])

        # 5) å…¶ä»–æ ‡é‡
        try:
            return float(surv_obj)
        except Exception:
            return float('nan')

    # åœ¨ class HCCSurvivalPredictor å†…æ–°å¢ä¸¤ä¸ªå·¥å…·æ–¹æ³•ï¼ˆæ”¾åœ¨ _survival_at ä¸‹æ–¹å³å¯ï¼‰
    def _pos_proba(self, model, proba: np.ndarray) -> float:
        """è¿”å›é˜³æ€§ç±»(æ ‡ç­¾=1/True)çš„æ¦‚ç‡ï¼Œå…¼å®¹ classes_ é¡ºåº"""
        if proba.ndim == 1:
            return float(proba[0])
        idx = 1
        if hasattr(model, "classes_"):
            cls = list(model.classes_)
            if 1 in cls:
                idx = cls.index(1)
            elif True in cls:
                idx = cls.index(True)
        return float(proba[0, idx])

    def _rsf_prob(self, model, X: pd.DataFrame, time_horizon: float) -> float:
        """ä¼˜å…ˆç”¨RSFçš„ç”Ÿå­˜å‡½æ•°åœ¨å›ºå®šæ—¶é—´ç‚¹Tå– 1-S(T)ï¼›å¦åˆ™é€€åŒ–ä¸º predict_proba æˆ–å†³ç­–åˆ†æ•°"""
        if hasattr(model, "predict_survival_function"):
            surv_funcs = model.predict_survival_function(X)
            s = self._survival_at(surv_funcs[0], time_horizon)
            return float(1.0 - s)
        if hasattr(model, "predict_proba"):
            proba = model.predict_proba(X)
            return self._pos_proba(model, proba)
        if hasattr(model, "predict"):
            yhat = model.predict(X)
            val = float(yhat[0]) if hasattr(yhat, "__getitem__") else float(yhat)
            return float(1.0 / (1.0 + np.exp(-val)))
        _fail("RSFæ¨¡å‹æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹æ¥å£")

    def _cox_prob(self, model, X: pd.DataFrame, time_horizon: float) -> float:
        """
        Coxæ¨¡å‹åœ¨å›ºå®šæ—¶é—´ç‚¹tçš„äº‹ä»¶æ¦‚ç‡
        :param model: å·²è®­ç»ƒçš„ lifelines.CoxPHFitter æ¨¡å‹
        :param X: æ ·æœ¬ç‰¹å¾ DataFrame
        :param time_horizon: å›ºå®šæ—¶é—´ç‚¹
        :return: æ¯ä¸ªæ ·æœ¬åœ¨ time_horizon çš„äº‹ä»¶æ¦‚ç‡
        """
        if hasattr(model, "predict_survival_function"):
            surv_funcs = model.predict_survival_function(X, times=[time_horizon])
            probs = 1.0 - surv_funcs.iloc[0].values
            return probs
        _fail("Coxæ¨¡å‹æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹æ¥å£")

    def ensure_feature_orders(self, sample_df: pd.DataFrame):
        """
        è‹¥ models/feature_order.json ä¸å­˜åœ¨æˆ–ç¼ºé”®ï¼Œåˆ™æ ¹æ®å·²åŠ è½½çš„æ¨¡å‹ä¸sample_dfç”Ÿæˆã€‚
        """
        json_path = "models/feature_order.json"
        need_build = True
        orders = {}
        if os.path.exists(json_path):
            try:
                orders = load_feature_orders(json_path)
                # æ£€æŸ¥å…³é”®æ¨¡å‹é”®æ˜¯å¦é½å…¨ï¼Œä¸é½åˆ™é‡å»º
                keys_needed = [k for k in ['cox', 'rsf', 'logistic', 'deepsurv', 'xgboost'] if
                               self.models.get(k) is not None]
                if all(k in orders for k in keys_needed):
                    need_build = False
            except Exception:
                need_build = True

        if need_build:
            orders = build_and_save_feature_orders(self.models, json_path, sample_df)

        self.feature_orders = orders

    def _align_X(self, df: pd.DataFrame, model, model_key: str) -> pd.DataFrame:
        """
        æŒ‰è®­ç»ƒæ—¶åˆ—é¡ºåºå¯¹é½ï¼Œè‹¥æ¨¡å‹æœ¬èº«å¸¦feature_names_in_åˆ™ä¼˜å…ˆç”¨ï¼›
        å¦åˆ™ä½¿ç”¨ feature_order.json ä¸­çš„å¯¹åº”é¡ºåºã€‚
        """
        if hasattr(model, "feature_names_in_"):
            cols = list(model.feature_names_in_)
        else:
            if not hasattr(self, "feature_orders") or model_key not in self.feature_orders:
                raise RuntimeError(f"ç¼ºå°‘ {model_key} çš„è®­ç»ƒåˆ—é¡ºåºï¼Œè¯·å…ˆç”Ÿæˆ models/feature_order.json")
            cols = self.feature_orders[model_key]

        missing = [c for c in cols if c not in df.columns]
        extra = [c for c in df.columns if c not in cols]
        if missing:
            raise RuntimeError(f"{model_key} ç¼ºå¤±ç‰¹å¾: {missing}")
        # å¯¹å¤šä½™ç‰¹å¾ä¸æŠ¥é”™ï¼Œç›´æ¥ä¸¢å¼ƒï¼Œç¡®ä¿ä¸è®­ç»ƒä¸€è‡´
        X = df[cols].astype("float32")
        return X

    def _predict_deepsurv(self, model, Xdf: pd.DataFrame) -> float:
        """DeepSurvç¨³å®šæ¨ç†ï¼šjoblib/sklearné£æ ¼æˆ–PyTorch Module"""
        # sklearn/joblib é£æ ¼
        if hasattr(model, "predict_proba"):
            proba = model.predict_proba(Xdf)
            if proba.ndim == 2 and proba.shape[1] >= 2:
                return float(proba[0, 1])
            return float(proba[0, 0])
        if hasattr(model, "predict") and not hasattr(model, "forward"):
            yhat = model.predict(Xdf)
            return float(yhat[0]) if hasattr(yhat, "__getitem__") else float(yhat)

        # PyTorch Module
        if hasattr(model, "forward"):
            import torch
            model.eval()
            with torch.no_grad():
                tensor = torch.as_tensor(Xdf.values, dtype=torch.float32)
                out = model(tensor)
                if isinstance(out, torch.Tensor):
                    out = out.squeeze()
                    out = out[0].item() if out.ndim else float(out.item())
                else:
                    out = float(out)
            # è‹¥æ˜¯é£é™©åˆ†æ•°ï¼Œç»Ÿä¸€ç»sigmoidæ˜ å°„ä¸º[0,1]ï¼›è‹¥ä½ çš„æ¨¡å‹æœ¬èº«è¾“å‡ºå·²æ˜¯æ¦‚ç‡ï¼Œå¯å»æ‰è¿™è¡Œ
            prob = 1.0 / (1.0 + np.exp(-out))
            return float(prob)

        _fail("DeepSurvæ¨¡å‹ä¸æ”¯æŒçš„ç±»å‹ï¼šç¼ºå°‘predict/predict_proba/forward")


    def _create_feature_mapping(self):
        """åˆ›å»ºä¸­æ–‡å˜é‡ååˆ°è‹±æ–‡çš„æ˜ å°„"""
        return {
            # åŸºæœ¬ä¿¡æ¯
            'å¹´é¾„__y': 'Age (Years)',
            'æ€§åˆ«_1': 'Gender (Male=1, Female=0)',

            # å®éªŒå®¤æ£€æŸ¥
            'ALB': 'ALB (g/L)',
            'AST': 'AST (U/L)',
            'ALT': 'ALT (U/L)',
            'TBIL': 'TBIL (umol/L)',
            'AFP': 'AFP (Î¼g/L)',
            'AFP_greater_400': 'AFP > 400',  # è¡ç”Ÿå˜é‡
            'AFP_less_400': 'AFP â‰¤ 400',
            'PT': 'PT (s)',
            'INR': 'INR',
            'WBC': 'WBC (10^9/L)',

            # ç–¾ç—…ç›¸å…³
            'å¤±è¡€é‡': 'Blood Loss (mL)',
            'è‚¿ç˜¤MVI_M0': 'Tumor MVI (M0=1, M1 or M2=0)',
            'è‚¿ç˜¤ç›´å¾„': 'Tumor Diameter (cm)',
            'æ˜¯å¦åˆå¹¶è‚ç¡¬åŒ–_1': 'Cirrhosis (present=1, absent=0)',
            'åŒ…è†œæ˜¯å¦å—ä¾µçŠ¯_æœªæµ¸åŠ': 'Capsule Invasion (present=1, absent=0)',
            'è‚¿ç˜¤æ˜¯å¦å·¨å—å‹åˆ†åŒ–_1': 'Tumor Differentiation (Massive=1, Others=0)',
            'æ˜¯å¦åˆå¹¶è‚ç‚_1': 'Hepatitis (present=1, absent=0)',
            'æ˜¯å¦å¤§èŒƒå›´åˆ‡é™¤_1': 'Wide Resection (yes=1, no=0)',
            'childåˆ†æœŸ_A': 'Child-Pugh Stage (A=1, B or C=0)',
            'ALBI': 'ALBI Score'
        }

    def _create_reverse_mapping(self):
        """åˆ›å»ºè‹±æ–‡åˆ°ä¸­æ–‡å˜é‡åçš„åå‘æ˜ å°„"""
        return {v: k for k, v in self.feature_mapping.items()}

    def _convert_afp_features(self, afp_value):
        """å°†AFPå€¼è½¬æ¢ä¸ºäºŒåˆ†ç±»ç‰¹å¾"""
        if afp_value > 400:
            return {'AFP_greater_400': 1, 'AFP_less_400': 0}
        else:
            return {'AFP_greater_400': 0, 'AFP_less_400': 1}

    def _map_chinese_to_english(self, feature_name):
        """å°†ä¸­æ–‡ç‰¹å¾åæ˜ å°„ä¸ºè‹±æ–‡"""
        return self.feature_mapping.get(feature_name, feature_name)

    def _map_english_to_chinese(self, english_name):
        """å°†è‹±æ–‡ç‰¹å¾åæ˜ å°„ä¸ºä¸­æ–‡"""
        return self.reverse_mapping.get(english_name, english_name)

    def load_models(self):
        """åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹"""
        try:
            # åŠ è½½éšæœºç”Ÿå­˜æ£®æ—æ¨¡å‹
            try:
                try:
                    self.models['rsf'] = joblib.load('models/newbest_rsf_model.pkl')
                    #st.sidebar.success("âœ… RSFæ¨¡å‹åŠ è½½æˆåŠŸ (joblib)")
                except:
                    with open('models/newbest_rsf_model.pkl', 'rb') as f:
                        self.models['rsf'] = pickle.load(f)
                    #st.sidebar.success("âœ… RSFæ¨¡å‹åŠ è½½æˆåŠŸ (pickle)")
            except Exception as e:
                st.sidebar.warning(f"âš ï¸ RSF Model loading failed: {str(e)}")
                self.models['rsf'] = self.create_mock_model('RSF')

        # åŠ è½½XGBoostæ¨¡å‹ (.ubjæ–‡ä»¶)
            if XGBOOST_AVAILABLE:
                        try:
                            self.models['xgboost'] = xgb.Booster()
                            self.models['xgboost'].load_model('models/aft_model.ubj')
                            #st.sidebar.success("âœ… XGBoostæ¨¡å‹åŠ è½½æˆåŠŸ")
                        except Exception as e:
                            st.sidebar.warning(f"âš ï¸ XGBoost Model loading failed: {str(e)}")
                            self.models['xgboost'] = self.create_mock_model('XGBoost')
            else:
                        st.sidebar.warning("âš ï¸ XGBoost not installed")
                        self.models['xgboost'] = self.create_mock_model('XGBoost')

                        # åŠ è½½Coxæ¨¡å‹ - å°è¯•å¤šç§åŠ è½½æ–¹å¼
            try:
                    self.models['cox'] = joblib.load('models/cox_model.pkl')
                    #st.sidebar.success("âœ… Coxæ¨¡å‹åŠ è½½æˆåŠŸ (joblib)")

            except Exception as e:
                st.sidebar.warning(f"âš ï¸ Cox Model loading failed: {str(e)}")
                self.models['cox'] = self.create_mock_model('Cox')

            try:
                    self.models['logistic'] = joblib.load('models/logistic_model.pkl')
                    #st.sidebar.success("âœ… LRæ¨¡å‹åŠ è½½æˆåŠŸ (joblib)")

            except Exception as e:
                st.sidebar.warning(f"âš ï¸ LR Model loading failed: {str(e)}")
                self.models['logistic'] = self.create_mock_model('logistic')

            try:
                    self.models['deepsurv'] = joblib.load('models/deepsurv_model.joblib')
                    ##st.sidebar.success("âœ… NNæ¨¡å‹åŠ è½½æˆåŠŸ (joblib)")

            except Exception as e:
                st.sidebar.warning(f"âš ï¸ NN Model loading failed: {str(e)}")
                self.models['deepsurv'] = self.create_mock_model('deepsurv')

        except Exception as e:
                st.error(f"âŒ Error: {str(e)}")

    def create_mock_model(self, model_name):
        class MockModel:
            def predict_proba(self, X):
                _fail(f"{model_name} æœªåŠ è½½æˆåŠŸï¼Œä¸”ç¦æ­¢ä½¿ç”¨éšæœºå…œåº•ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚")

            def predict(self, X):
                _fail(f"{model_name} æœªåŠ è½½æˆåŠŸï¼Œä¸”ç¦æ­¢ä½¿ç”¨éšæœºå…œåº•ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ã€‚")

        return MockModel(model_name)

    def preprocess_input(self, input_data):
        return input_data.copy()

    def check_rsf_model(self):
        """æ£€æŸ¥RSFæ¨¡å‹çš„å…·ä½“é—®é¢˜"""
        if self.models['rsf'] is not None:
            model = self.models['rsf']
            st.write("**RSFæ¨¡å‹è¯¦ç»†ä¿¡æ¯:**")

            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            st.write(f"æ¨¡å‹ç±»å‹: {type(model)}")
            st.write(f"æ¨¡å‹ç±»å: {model.__class__.__name__}")

            # æ£€æŸ¥æ¨¡å‹å±æ€§
            if hasattr(model, 'n_features_in_'):
                st.write(f"è¾“å…¥ç‰¹å¾æ•°: {model.n_features_in_}")

            if hasattr(model, 'n_outputs_'):
                st.write(f"è¾“å‡ºæ•°: {model.n_outputs_}")

            if hasattr(model, 'estimators_'):
                st.write(f"æ ‘çš„æ•°é‡: {len(model.estimators_)}")

            # æ£€æŸ¥é¢„æµ‹æ–¹æ³•
            if hasattr(model, 'predict_proba'):
                st.write("æ”¯æŒpredict_proba")
            if hasattr(model, 'predict'):
                st.write("æ”¯æŒpredict")
            if hasattr(model, 'predict_survival_function'):
                st.write("æ”¯æŒpredict_survival_function")

            # å°è¯•è·å–ç‰¹å¾é‡è¦æ€§
            if hasattr(model, 'feature_importances_'):
                st.write(f"ç‰¹å¾é‡è¦æ€§å½¢çŠ¶: {model.feature_importances_.shape}")

    def predict_survival(self, input_data):
        """ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆä¸¥æ ¼æŒ‰è®­ç»ƒåˆ—é¡ºåºå¯¹é½ï¼›æ— éšæœºå…œåº•ï¼›ç¨³å®šè¾“å‡ºï¼‰"""
        processed_data = self.preprocess_input(input_data)

        # ç”Ÿæˆ/åŠ è½½è®­ç»ƒåˆ—é¡ºåºï¼ˆmodels/feature_order.jsonï¼‰
        self.ensure_feature_orders(processed_data)

        predictions = {}

        # RSF
        if self.models.get('rsf') is not None:
            mr = self.models['rsf']
            X = self._align_X(processed_data, mr, "rsf")
            prob = self._rsf_prob(mr, X, self.time_horizon)
            predictions['Random Survival Forest'] = np.clip(float(prob), 0.0, 1.0)

        if self.models.get('cox') is not None:
            mc = self.models['cox']
            X = self._align_X(processed_data, mc, "cox")
            # å‡è®¾ self.baseline_surv æ˜¯å·²ç»è®¡ç®—å¥½çš„ S0(time_horizon)
            prob = self._cox_prob(mc,X,self.time_horizon)
            predictions['Cox-PH'] = np.clip(float(prob), 0.0, 1.0)

        # --- XGBoost AFT æ¨¡å‹é¢„æµ‹ ---
        if self.models.get("xgboost") is not None:
            m_xgb = self.models["xgboost"]
            scaler = joblib.load("xgb_scaler.pkl")
            feature_cols = ['PT',
                            'childåˆ†æœŸ_A', 'AFP_greater_400',
                            'å¤±è¡€é‡',
                            'è‚¿ç˜¤æ˜¯å¦å·¨å—å‹åˆ†åŒ–_1', 'AST', 'ALT', 'WBC', 'INR', 'TBIL',
                            'å¹´é¾„__y',
                            'æ€§åˆ«_1', 'ALBI', 'åŒ…è†œæ˜¯å¦å—ä¾µçŠ¯_æœªæµ¸åŠ', 'è‚¿ç˜¤ç›´å¾„',
                            'ALB', 'æ˜¯å¦åˆå¹¶è‚ç¡¬åŒ–_1',
                            'æ˜¯å¦å¤§èŒƒå›´åˆ‡é™¤_1',
                            'æ˜¯å¦åˆå¹¶è‚ç‚_1', 'è‚¿ç˜¤MVI_M0',
                            'AFP_less_400']
            X_valid = processed_data[feature_cols]

            X_aligned = scaler.transform(X_valid)

            # æ„é€  DMatrix
            dmatrix = xgb.DMatrix(X_aligned)

            # é¢„æµ‹ç”Ÿå­˜æ—¶é—´ï¼ˆAFT è¾“å‡ºï¼‰
            pred_time = m_xgb.predict(dmatrix)  # è¿”å› array([xxx])

            # è¡¨æ ¼æ˜¾ç¤ºï¼šåªæ˜¾ç¤ºé¢„æµ‹ç”Ÿå­˜æœŸï¼Œä¸å½±å“æ¦‚ç‡æ¨¡å—
            display_value = float(pred_time[0])
            predictions["XGBoost"] = display_value

        if self.models.get("logistic") is not None:
            clf, scaler = self.models["logistic"]

            # å¯¹é½ç‰¹å¾
            available_features = [
                'PT', 'childåˆ†æœŸ_A', 'AFP_greater_400', 'å¤±è¡€é‡',
                'è‚¿ç˜¤æ˜¯å¦å·¨å—å‹åˆ†åŒ–_1', 'AST', 'ALT', 'WBC', 'INR', 'TBIL',
                'å¹´é¾„__y', 'æ€§åˆ«_1', 'ALBI', 'åŒ…è†œæ˜¯å¦å—ä¾µçŠ¯_æœªæµ¸åŠ', 'è‚¿ç˜¤ç›´å¾„',
                'ALB', 'æ˜¯å¦åˆå¹¶è‚ç¡¬åŒ–_1', 'æ˜¯å¦å¤§èŒƒå›´åˆ‡é™¤_1', 'æ˜¯å¦åˆå¹¶è‚ç‚_1',
                'è‚¿ç˜¤MVI_M0', 'AFP_less_400'
            ]
            X_aligned = processed_data[available_features].values
            X_scaled = scaler.transform(X_aligned)

            # é¢„æµ‹ 730 å¤©å†…å¤å‘/æ­»äº¡çš„æ¦‚ç‡
            proba = clf.predict_proba(X_scaled)[:, 1]
            pred_proba = float(proba[0])

            # æ˜¾ç¤º
            display_value_lr = float(pred_proba)
            predictions["Logistic Regression"] = display_value_lr

        if self.models.get("deepsurv") is not None:
            wrapper = self.models["deepsurv"]
            available_features = [
                'PT', 'childåˆ†æœŸ_A', 'AFP_greater_400', 'å¤±è¡€é‡',
                'è‚¿ç˜¤æ˜¯å¦å·¨å—å‹åˆ†åŒ–_1', 'AST', 'ALT', 'WBC', 'INR', 'TBIL',
                'å¹´é¾„__y', 'æ€§åˆ«_1', 'ALBI', 'åŒ…è†œæ˜¯å¦å—ä¾µçŠ¯_æœªæµ¸åŠ', 'è‚¿ç˜¤ç›´å¾„',
                'ALB', 'æ˜¯å¦åˆå¹¶è‚ç¡¬åŒ–_1', 'æ˜¯å¦å¤§èŒƒå›´åˆ‡é™¤_1', 'æ˜¯å¦åˆå¹¶è‚ç‚_1',
                'è‚¿ç˜¤MVI_M0', 'AFP_less_400'
            ]
            # å¯¹é½ç‰¹å¾
            X_aligned = processed_data[available_features].values
            # é¢„æµ‹æ¦‚ç‡
            probs = wrapper.predict_proba(X_aligned)[0]  # å¯èƒ½æ˜¯ä¸€ä¸ªæ›²çº¿ (array)
            # å–æŸä¸ªæ—¶é—´ç‚¹ï¼Œæ¯”å¦‚ç¬¬12ä¸ªæœˆï¼ˆå‡è®¾æ¯åˆ—æ˜¯æœˆï¼‰
            display_value_nn = float(probs[1])
            predictions["Neural Network"] = display_value_nn

        return predictions, display_value, processed_data, mr, mc, m_xgb, wrapper,clf

# åˆå§‹åŒ–é¢„æµ‹å™¨
@st.cache_resource
def load_predictor():
    return HCCSurvivalPredictor()


predictor = load_predictor()

# åº”ç”¨æ ‡é¢˜
st.markdown("<h1 style='text-align: center;'>HCC Recurrence-Free Survival Analysis Prediction System</h1>", unsafe_allow_html=True)
st.markdown("<h3 style='text-align: center; color: red;'>Disclaimer: Only used for research purposes</h3>", unsafe_allow_html=True)

# ä¾§è¾¹æ  - è¾“å…¥å˜é‡
with st.sidebar:
    st.header("Info Input")

    # åŸºæœ¬ä¿¡æ¯
    st.subheader("Basic Info")
    age = st.number_input("Age (Years)", min_value=18, max_value=100, value=47)
    gender = st.number_input("Gender (Male=1, Female=0)",min_value=0, max_value=1, value=1, step=1
                                )

    # å®éªŒå®¤æ£€æŸ¥
    st.subheader("Laboratory indicators")
    alb = st.number_input("ALB (g/L)", min_value=20.0, max_value=60.0, value=42.3, step=0.1)
    ast = st.number_input("AST (U/L)", min_value=5.0, max_value=500.0, value=30.0, step=1.0)
    tbil = st.number_input("TBIL (umol/L)", min_value=2.0, max_value=200.0, value=11.0, step=0.1)
    albi = st.number_input("ALBI", min_value=-3.0, max_value=0.0, value=-2.91, step=0.1)
    alt = st.number_input("ALT (U/L)", min_value=5.0, max_value=500.0, value=42.0, step=1.0)  # æ·»åŠ ALT
    afp = st.number_input("AFP (Î¼g/L)", min_value=0.0, max_value=10000.0, value=600.0, step=1.0)

    # æ˜¾ç¤ºAFPè½¬æ¢ç»“æœ
    afp_features = predictor._convert_afp_features(afp)
    # if afp > 400:
    #     st.info(f"AFP = {afp} Î¼g/L â†’ AFP_greater_400 = 1, AFP_less_400 = 0")
    # else:
    #     st.info(f"AFP = {afp} Î¼g/L â†’ AFP_greater_400 = 0, AFP_less_400 = 1")

    # å…¶ä»–å®éªŒå®¤æŒ‡æ ‡
    pt = st.number_input("PT (s)", min_value=8.0, max_value=30.0, value=11.5, step=0.1)
    inr = st.number_input("INR", min_value=0.8, max_value=5.0, value=0.97, step=0.1)
    wbc = st.number_input("WBC (10^9/L)", min_value=1.0, max_value=50.0, value=8.88, step=0.1)

    st.subheader("Surgery")
    blood_loss = st.number_input("Blood Loss (mL)", min_value=0, max_value=5000, value=500, step=50)

    # ä¿®æ”¹å¤§èŒƒå›´åˆ‡é™¤ä¸º0/1è¾“å…¥
    wide_resection = st.number_input("Extensive Resection (yes=1, no=0)", min_value=0, max_value=1, value=1, step=1
                                     )
    st.subheader("Pathology")
    # ä¿®æ”¹MVIä¸º0/1è¾“å…¥
    tumor_mvi = st.number_input("Tumor MVI (M0=1, M1 or M2=0)", min_value=0, max_value=1, value=0, step=1)

    tumor_diameter = st.number_input("Tumor Diameter(cm)", min_value=0.1, max_value=20.0, value=4.0, step=0.1)
    # ä¿®æ”¹è‚ç‚ä¸º0/1è¾“å…¥
    hepatitis = st.number_input("Hepatitis (present=1, absent=0)", min_value=0, max_value=1, value=1, step=1
                                )
    # ä¿®æ”¹è‚ç¡¬åŒ–ä¸º0/1è¾“å…¥
    cirrhosis = st.number_input("Cirrhosis (present=1, absent=0)", min_value=0, max_value=1, value=0, step=1,
                                )

    # ä¿®æ”¹åŒ…è†œä¾µçŠ¯ä¸º0/1è¾“å…¥
    capsule_invasion = st.number_input("Capsule Invasion (present=1, absent=0)", min_value=0, max_value=1, value=1,
                                       step=1)

    # ä¿®æ”¹è‚¿ç˜¤åˆ†åŒ–ä¸º0/1è¾“å…¥
    tumor_differentiation = st.number_input("Tumor Differentiation (giant mass type=1, Others=0)", min_value=0, max_value=1,
                                            value=0, step=1)

    # ä¿®æ”¹Child-Pughåˆ†æœŸä¸º0/1è¾“å…¥
    child_stage = st.number_input("Child-Pugh Stage (A=1, B or C=0)", min_value=0, max_value=1, value=1, step=1,
                                 )

    # é¢„æµ‹æŒ‰é’®
    predict_button = st.button("ğŸš€ Start Prediction", type="primary", use_container_width=True)

    # é‡ç½®æŒ‰é’®
    if st.button("ğŸ”„ Reset Input", use_container_width=True):
        st.rerun()

# ä¸»ç•Œé¢
if predict_button:
    st.success("âœ… Generating Results...")

    # åˆ›å»ºè¾“å…¥æ•°æ®æ¡† - ä½¿ç”¨æ¨¡å‹ä¸­çš„ä¸­æ–‡å˜é‡å
    input_data = pd.DataFrame({
        'å¹´é¾„__y': [age],
        'æ€§åˆ«_1': [gender],
        'ALB': [alb],
        'AST': [ast],
        'ALT': [alt],  # æ·»åŠ ALT
        'TBIL': [tbil],
        # 'AFP': [afp],
        'PT': [pt],
        'INR': [inr],
        'WBC': [wbc],
        'ALBI': [albi],
        'å¤±è¡€é‡': [blood_loss],
        'è‚¿ç˜¤MVI_M0': [tumor_mvi],
        'è‚¿ç˜¤ç›´å¾„': [tumor_diameter],
        'æ˜¯å¦åˆå¹¶è‚ç¡¬åŒ–_1': [cirrhosis],
        'åŒ…è†œæ˜¯å¦å—ä¾µçŠ¯_æœªæµ¸åŠ': [capsule_invasion],
        'è‚¿ç˜¤æ˜¯å¦å·¨å—å‹åˆ†åŒ–_1': [tumor_differentiation],
        'æ˜¯å¦åˆå¹¶è‚ç‚_1': [hepatitis],
        'æ˜¯å¦å¤§èŒƒå›´åˆ‡é™¤_1': [wide_resection],
        'childåˆ†æœŸ_A': [child_stage]
    })
    # æ·»åŠ AFPè½¬æ¢åçš„ä¸¤ä¸ªè¡ç”Ÿå˜é‡
    afp_features = predictor._convert_afp_features(afp)
    input_data['AFP_greater_400'] = [afp_features['AFP_greater_400']]
    input_data['AFP_less_400'] = [afp_features['AFP_less_400']]
    # æ˜¾ç¤ºè¾“å…¥æ•°æ®
    st.subheader("Input Confirmation")
    col1, col2 = st.columns(2)

    with col1:
        st.write("**Basic Info:**")
        st.write(f"- Age: {age} ")
        st.write(f"- Gender (Male=1, Female=0): {gender}")
        st.write(f"- ALB: {alb} g/L")
        st.write(f"- AST: {ast} U/L")
        st.write(f"- ALT: {alt} U/L")  # æ·»åŠ ALTæ˜¾ç¤º
        st.write(f"- TBIL: {tbil} umol/L")
        st.write(f"- ALBI Score: {albi}")
        st.write(f"- AFP: {afp} Î¼g/L")
        st.write(f"- AFP > 400 Î¼g/L: {afp_features['AFP_greater_400']}")  # æ˜¾ç¤ºè¡ç”Ÿå˜é‡
        st.write(f"- AFP â‰¤ 400 Î¼g/L: {afp_features['AFP_less_400']}")  # æ˜¾ç¤ºè¡ç”Ÿå˜é‡
        st.write(f"- PT: {pt} s")
        st.write(f"- INR: {inr}")

    with col2:
        st.write("**Disease and Surgery:**")
        st.write(f"- WBC: {wbc} Ã—10^9/L")
        st.write(f"- Blood Loss: {blood_loss} mL")
        st.write(f"- Tumor MVI (M0=1, M1 or M2=0): {tumor_mvi}")
        st.write(f"- Tumor Diameter: {tumor_diameter} cm")
        st.write(f"- Cirrhosis (present=1, absent=0): {cirrhosis}")
        st.write(f"- Capsule Invasion (present=1, absent=0): {capsule_invasion}")
        st.write(f"- Tumor Differentiation (Massive=1, Others=0): {tumor_differentiation}")
        st.write(f"- Hepatitis (present=1, absent=0): {hepatitis}")
        st.write(f"- Wide Resection (yes=1, no=0): {wide_resection}")
        st.write(f"- Child-Pugh Stage (A=1, B or C=0): {child_stage}")

    # # æ˜¾ç¤ºAFPè½¬æ¢ç»“æœ
    # st.subheader(" AFPæŒ‡æ ‡è½¬æ¢ç»“æœ")
    # afp_features = predictor._convert_afp_features(afp)
    # afp_df = pd.DataFrame({
    #     'ç‰¹å¾å': ['AFP_greater_400', 'AFP_less_400'],
    #     'æ•°å€¼': [afp_features['AFP_greater_400'], afp_features['AFP_less_400']],
    #     'è¯´æ˜': [
    #         f"AFP > 400 Î¼g/L (å½“å‰å€¼: {afp})",
    #         f"AFP â‰¤ 400 Î¼g/L (å½“å‰å€¼: {afp})"
    #     ]
    # })
    # st.dataframe(afp_df, use_container_width=True)

    st.markdown("---")

    # æ¨¡å‹é¢„æµ‹éƒ¨åˆ†
    st.subheader("Results")

    # è·å–é¢„æµ‹ç»“æœ
    # è·å–é¢„æµ‹ç»“æœï¼ˆä¸åŒ…å«XGBoostï¼‰
    predictions, display_value, processed_data, mr, mc, m_xgb, wrapper,ml = predictor.predict_survival(input_data)
    # åˆ é™¤æˆ–å¿½ç•¥ XGBoost é¢„æµ‹
    if "XGBoost" in predictions:
        del predictions["XGBoost"]
    # åˆ›å»ºé¢„æµ‹ç»“æœå±•ç¤º
    col1, col2 = st.columns([2, 1])

    with col1:
        # æ¨¡å‹é¢„æµ‹ç»“æœè¡¨æ ¼
        st.write("**Risk Probability of Each Model:**")


        def get_risk_level(risk_prob):
            """æ ¹æ®é£é™©æ¦‚ç‡ç¡®å®šé£é™©ç­‰çº§"""
            if risk_prob >= 0.7:
                return 'High risk'
            elif risk_prob >= 0.4:
                return 'Medium risk'
            else:
                return 'Low risk'


        results_df = pd.DataFrame({
            'Model': list(predictions.keys()),
            'Risk': list(predictions.values()),
            'Risk Level': [get_risk_level(risk) for risk in predictions.values()]
        })
        # æ·»åŠ é¢œè‰²ç¼–ç 
        def color_risk(val):
            if val == 'High risk':
                return 'background-color: #ffcdd2'
            elif val == 'Medium risk':
                return 'background-color: #fff3e0'
            else:
                return 'background-color: #c8e6c9'


        styled_df = results_df.style.applymap(
            lambda x: color_risk(x) if isinstance(x, str) else '',
            subset=['Risk Level']
        )
        st.dataframe(styled_df, use_container_width=True)

        # é£é™©æ¦‚ç‡å¯è§†åŒ–
        fig = px.bar(
            x=list(predictions.keys()),
            y=list(predictions.values()),
            title="Comparison of Risk Probability",
            labels={'x': 'Model', 'y': 'Risk'},
            color=list(predictions.values()),
            color_continuous_scale='RdYlGn_r'
        )
        fig.update_traces(hovertemplate="Model: %{x}<br>Risk: %{y:.6f}")
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # Comprehensive Risk Assessment
        st.write("**Comprehensive Risk Assessment:**")
        avg_risk = np.mean(list(predictions.values()))
        overall_risk = get_risk_level(avg_risk)

        if overall_risk == 'High Risk':
            st.error(f"âš ï¸ Overall Risk: {overall_risk}")
            st.write(f"Average Risk Probability: {avg_risk:.3f}")
            st.write("Recommendation: Close monitoring and active treatment are advised.")
        elif overall_risk == 'Medium Risk':
            st.warning(f"âš ï¸ Overall Risk: {overall_risk}")
            st.write(f"Average Risk Probability: {avg_risk:.3f}")
            st.write("Recommendation: Regular follow-up and careful observation are recommended.")
        else:
            st.success(f"âœ… Overall Risk: {overall_risk}")
            st.write(f"Average Risk Probability: {avg_risk:.3f}")
            st.write("Recommendation: Continue monitoring and maintain current status.")

        # æ¨¡å‹ä¸€è‡´æ€§åˆ†æ
        st.write("**Model Consistency:**")
        risk_std = np.std(list(predictions.values()))
        if risk_std < 0.1:
            st.success("Model predictions are highly consistent")
        elif risk_std < 0.2:
            st.warning("The model predictions are basically consistent")
        else:
            st.error("Model predictions differ")

        st.write(f"Risk Standard Deviation: {risk_std:.3f}")

    st.markdown("---")

    # è¯¦ç»†åˆ†æ
    st.subheader("Survival analysis")
    # st.write("**é¢„æµ‹ç”Ÿå­˜æ›²çº¿ä¸XGBoosté¢„æµ‹æ—¶é—´:**")

    # åˆ›å»ºä¸¤åˆ—
    col1, col2 = st.columns([3, 1])  # col1å 3/4å®½åº¦ï¼Œcol2å 1/4å®½åº¦

    # --- å·¦è¾¹: ç”Ÿå­˜æ›²çº¿ ---
    with col1:

        from survcurve import plot_survival_curves
        muse = joblib.load('models/deepsurv_strict.joblib')
        fig = plot_survival_curves(
            processed_data,
            cox_model=mc,
            rsf_model=mr,
            xgb_aft_model=m_xgb,
            deepsurv_model=muse
        )

        st.plotly_chart(fig, use_container_width=True)

    # --- å³è¾¹: XGBoosté¢„æµ‹æ—¶é—´ ---
    with col2:
        if predictor.models.get("xgboost") is not None:
            st.subheader("XGBoost Predicted Recurrence-Free Survival")
            st.info(f"Predicted RFS: {int(display_value)} days")

    # å¯¼å‡ºç»“æœ
    st.markdown("---")
    st.subheader("Result Export")

    # ç”ŸæˆæŠ¥å‘Š
    # ç”ŸæˆæŠ¥å‘Š
    report_data = {
        'Prediction time': [datetime.now().strftime("%Y-%m-%d %H:%M:%S")],
        'Age': [age],
        'Gender (1=Male,0=Female)': [gender],
        'ALBI': [albi],
        # 'AFP': [afp],
        # 'AFP_greater_400': [afp_features['AFP_greater_400']],
        # 'AFP_less_400': [afp_features['AFP_less_400']],
        'Tumor Diameter (cm)': [tumor_diameter],
        # 'Cirrhosis (1=present )': [cirrhosis],
        'Average risk': [avg_risk],
        'Overall risk level': [overall_risk],
        #'æ¨¡å‹ä¸€è‡´æ€§': ['ä¸€è‡´' if risk_std < 0.1 else 'å­˜åœ¨åˆ†æ­§']
    }

    report_df = pd.DataFrame(report_data)

    col1, col2 = st.columns(2)

    with col1:
        st.download_button(
            label="ğŸ“¥ Download Prediction Report(CSV)",
            data=report_df.to_csv(index=False),
            file_name=f"hcc_survival_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

    with col2:
        # åˆ›å»ºè¯¦ç»†ç»“æœ
        detailed_results = pd.DataFrame({
            'Model': list(predictions.keys()),
            'Risk': list(predictions.values()),
            'Risk Level': [get_risk_level(risk) for risk in predictions.values()]
        })

        st.download_button(
            label="ğŸ“¥ Download all model predictions (CSV)",
            data=detailed_results.to_csv(index=False),
            file_name=f"detailed_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

else:
    # åˆå§‹ç•Œé¢
    st.info("Please enter the patient information on the left and click the 'Start Prediction' button")

    # # æ˜¾ç¤ºåº”ç”¨è¯´æ˜
    # st.subheader("Notes")
    # st.write('View Results: Including the 2-year recurrence-free survival probability predicted from 4 mod ')

    # col1, col2 = st.columns(2)
    #
    # with col1:
    #     st.write("**åŠŸèƒ½ç‰¹ç‚¹:**")
    #     st.write("â€¢ é›†æˆ5ä¸ªç”Ÿå­˜åˆ†ææ¨¡å‹")
    #     st.write("â€¢ å®æ—¶é£é™©æ¦‚ç‡è®¡ç®—")
    #     st.write("â€¢ å¤šç»´åº¦é£é™©è¯„ä¼°")
    #     st.write("â€¢ ç»“æœå¯è§†åŒ–å±•ç¤º")
    #     st.write("â€¢ æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½")
    #     st.write("â€¢ æ”¯æŒä¸­æ–‡å˜é‡åæ˜ å°„")
    #     st.write("â€¢ AFPæŒ‡æ ‡è‡ªåŠ¨è½¬æ¢")
    #
    # with col2:
    #     st.write("**ä½¿ç”¨æ­¥éª¤:**")
    #     st.write("1. è¾“å…¥æ‚£è€…åŸºæœ¬ä¿¡æ¯")
    #     st.write("2. å¡«å†™ä¸´åºŠæŒ‡æ ‡")
    #     st.write("3. é€‰æ‹©ç–¾ç—…ç‰¹å¾")
    #     st.write("4. ç‚¹å‡»é¢„æµ‹æŒ‰é’®")
    #     st.write("5. æŸ¥çœ‹é¢„æµ‹ç»“æœ")
    #     st.write("6. ä¸‹è½½é¢„æµ‹æŠ¥å‘Š")
    #
    # # æ¨¡å‹ä¿¡æ¯
    # st.subheader("ğŸ¤– æ¨¡å‹ä¿¡æ¯")
    # model_info = pd.DataFrame({
    #     'æ¨¡å‹åç§°': ['Coxæ¯”ä¾‹é£é™©æ¨¡å‹', 'éšæœºç”Ÿå­˜æ£®æ—', 'é€»è¾‘å›å½’', 'XGBoostæ¨¡å‹', 'æ·±åº¦ç”Ÿå­˜ç½‘ç»œ'],
    #     'æ¨¡å‹ç±»å‹': ['ç»Ÿè®¡æ¨¡å‹', 'æœºå™¨å­¦ä¹ ', 'ç»Ÿè®¡æ¨¡å‹', 'æ¢¯åº¦æå‡', 'æ·±åº¦å­¦ä¹ '],
    #     'é€‚ç”¨åœºæ™¯': ['çº¿æ€§å…³ç³»', 'éçº¿æ€§å…³ç³»', 'äºŒåˆ†ç±»', 'å¤æ‚æ¨¡å¼', 'ç‰¹å¾å­¦ä¹ '],
    #     'ä¼˜åŠ¿': ['è§£é‡Šæ€§å¼º', 'å¤„ç†ç¼ºå¤±å€¼', 'è®¡ç®—ç®€å•', 'é¢„æµ‹ç¨³å®š', 'è‡ªåŠ¨ç‰¹å¾']
    # })
    # st.dataframe(model_info, use_container_width=True)

    # AFPè½¬æ¢è¯´æ˜
    st.subheader("AFP Conversion Instructions")
    st.write("""
    **AFP's Automatic Conversion Rules:**
    - When AFP > 400 Î¼g/Lï¼šAFP_greater_400 = 1, AFP_less_400 = 0
    - When AFP â‰¤ 400 Î¼g/Lï¼šAFP_greater_400 = 0, AFP_less_400 = 1

    **Example:**
    - AFP = 500 â†’ AFP_greater_400 = 1, AFP_less_400 = 0
    - AFP = 400 â†’ AFP_greater_400 = 0, AFP_less_400 = 1
    - AFP = 200 â†’ AFP_greater_400 = 0, AFP_less_400 = 1
    """)

    # å˜é‡åæ˜ å°„è¯´æ˜